\section*{Seijin's Snippets}

\subsection{Blending overlapping patches}

In this first method, the image is divided into fully-overlapping patches [TODO FIG 1 (PATCH OVERLAP.jpeg)]., patches on which sparse-coding is performed individually. Each missing pixel of the picture is hence treated four times by sparse-coding overall, as every pixel is covered by four overlapping patches as shown in figure 1. 
The exchange of information over the individually-filled masks takes place at moment of the reconstruction of the whole picture from these four candidates. Since each one of the four patches filled the missing values given a different surrounding, some may be performing better than others. Hence, with a proper reconstruction method, we can take the best out of the additional information provided by overlapping.
One intuitive idea would be to take a weighted average of the four versions. One could think of taking a uniform weight, or a gradient shift of weight from one patch to another. In our model, we opted for a case-dependent weighing that aims before anything to improve the visual perception of the quality of the inpainting. 
Our eyes are in general very sensitive to contrast. Hence, in order to improve the perceived quality of an inpainting, one needs to avoid a reconstruction that presents strong gradients in luminosity at the border of an inpainted area. This motivation lead to our blending method, which decides the weight distribution for every pixel, in order to locally minimize any jump in luminosity across a mask border.
Given the four versions of reconstruction, we calculate for each one of them the absolute-gradient on every pixel at the border of the inpainted mask. Intuitively, the score for each pixel at the border represent the un-confidence in the correctness of the inpainting at this location, since there should not be a strong jump in luminosity from one side of the border to another. For each of the pixels, we then diffuse its un-confidence score to all of its neighboring pixels with a decreasing factor, such that we obtain in the end of the process, an un-confidence score for every inpainted pixels, for each of the four versions of reconstruction. The weight distribution is then defined as the inverse of the exponential of the un-confidence score.
[TODO: INSERT PICTURES 1(BEFORE.png) 2(AFTER.png) 3(WEIGHT.jpeg)]

1: Result of the uniform average of the four versions of inpainting. We notice some artifacts in the shape of the mask, due to the unnatural gradient at its border.
2: Result of our blending method. Some artifacts almost completely disappeared.
3: A picture showing the un-confidence score of inpainted pixels. Black pixels represent low confidence. Such a picture is produced for each of the four inpainting results.

\subsection{Dictionary learning}

Another direction in which we investigated is the construction of the dictionary we would use for our sparse coding. The nature of a dictionary not only determines the quality of the inpainting result, but also the speed of the inpainting process, depending on the size of the dictionary as well as the degree of sparseness reachable with it. 
The choice of dictionary was particularly of interest in our case to improve the running time. The approach we adopted for the reconstruction of the image would a priori assure a good quality of the reconstruction, however on the expense of the time necessary for the reconstruction, hence the need of a dictionary which would limit this trade-off without affecting too much the improved quality of the reconstruction.
With this objective in mind, we built a under-complete dictionary on the space of patches of size 16x16, composed of 64 atoms (25 percent of the total dimension), by applying the approximate K-SVD algorithm on a training data customized for our need. We also paid particular attention to the initialization of the learning process, since this method is known to be initialization-sensitive [TODO REF]. In the rest of this section we will refer to the space of patches of size 16x16 as $E$.
\mypar{K-SVD algorithm}
The algorithm takes a training set of patches $X$, and learn a dictionary $U$, starting from an initialized value of $U$ and iteratively learning to fit to $X$ in a more efficient way. 
For all dictionaries, we used this algorithm with 15 iterations, using the same training data set.
\mypar{Training Data set}
The dictionary being under-complete, loss of information will inevitably occur. The atoms will hence need to be very efficient in reconstructing the image without losing important information. In other words, the atoms need to be able to describe characteristics of patches that have a high-variability among the kind of patches that we want to reconstruct. The aspects of patches with low-variability can be easily estimated by the mean, with a minimal loss of information. 
To achieve this, we first gathered patches of size 16x16 extracted from 36 pictures of size 512x512, consisting of photography of cities, nature and animals. We then applied principal component analysis (PCA) on these patches to deduce the most significant subspace of $E$ of dimension 64, i.e. the subspace of $E$ that include the highest variability of value among patches of real-world photography. We then projected the whole training set as well as our learning process onto that subspace. That way, we ensure that the learned 64 atoms stay in this a priori significant subspace, without getting lost into the less-meaningful directions of $E$, allowing them to efficiently reconstruct a patch.
\mypar{Initialization}
The algorithm we chose is sensitive to the choice of the initial candidate, it optimizes it locally and greedily until no progress is possible. We implemented 2 main different ways to initialize the learning process:
\begin{itemize}
   \item From a known dictionary: in this method, we initialize the dictionary on a known dictionary, such as a 64-atom DCT (Discrete Cosine Transform) dictionary, or the 64 significant vectors obtained by applying PCA to the training set.  
   \item k-means initialization: in this method, we initialize the dictionary based on the training data set. We first center and normalize the training data, after which we apply the K-means algorithm with K=64 based on the euclidean norm, and use the normalized 64 centroids as initializing atoms. To improve the robustness of the K-mean algorithm, we applied the algorithm 10 times and chose the result which had the smallest mean pairwise-distance with other clustering. 
\end{itemize}
A randomly-generated atom initialization have also been implemented to serve as a reference for studying the efficiency of other initialization methods. 

Lastly, to compare the efficiency of the overall dictionary performance, we set the baseline reference as the standard complete DCT dictionary.